# LLM Gateway Implementation - April 7, 2025

## Part 1: Llama Streaming Implementation Fix

### Issue
The Llama streaming implementation was experiencing an infinite loop where the same request was being sent to Bedrock repeatedly. This was causing the model to generate repetitive responses and creating unnecessary API calls.

### Root Cause
The issue stemmed from a circular dependency in our streaming implementation:

1. `MetaVendor.streamProcess` was calling `provider.streamChat`
2. `BedrockProvider.streamChat` was calling back into `vendor.streamProcess`
3. This created an infinite loop of requests

### Solution
We fixed this by:

1. Moving the Bedrock streaming logic directly into the `MetaVendor` class
2. Using the AWS SDK's `InvokeModelWithResponseStreamCommand` directly
3. Properly formatting the prompt according to Llama 3's instruction format:
   ```typescript
   const formattedPrompt = `<|begin_of_text|><|start_header_id|>user<|end_header_id|>
   ${userMessage.content}
   <|eot_id|>
   <|start_header_id|>assistant<|end_header_id|>`;
   ```

### Key Learnings

1. **Vendor-Specific Logic**: Streaming implementations should be vendor-specific and not rely on provider-level abstractions for streaming. Each vendor may have unique requirements for:
   - Prompt formatting
   - Response handling
   - Chunk processing

2. **Avoid Circular Dependencies**: When implementing streaming:
   - Keep the streaming logic in one place (either provider or vendor)
   - Don't create circular calls between provider and vendor
   - Use direct AWS SDK calls when needed

3. **Proper Prompt Formatting**: Llama 3 requires specific instruction format markers:
   - `<|begin_of_text|>` to start
   - `<|start_header_id|>user<|end_header_id|>` for user messages
   - `<|eot_id|>` to end user messages
   - `<|start_header_id|>assistant<|end_header_id|>` to start assistant response

4. **Streaming State Management**: 
   - Each vendor should manage its own streaming state
   - Don't try to track state across provider/vendor boundaries
   - Let the AWS SDK handle the streaming mechanics

## Part 2: Conversation Repository Implementation

### Overview
We implemented a comprehensive conversation management system to enable stateful interactions with LLMs. This allows users to start conversations and continue them later, with the LLM having access to the full conversation history.

### Components Implemented

1. **ConversationRepository Interface**:
   - Defined a standard interface for conversation storage operations
   - Supported operations: create, get, add messages, list, delete

2. **DynamoConversationRepository**:
   - Implemented the repository using DynamoDB for persistence
   - Used an efficient data model with composite keys:
     - Partition key: `userId`
     - Sort key: `CONVERSATION#{id}#MSG#{sequenceNumber}` for messages
     - Sort key: `CONVERSATION#{id}#META` for metadata
   - Implemented optimized batch operations for message handling

3. **DynamoDB Client Utilities**:
   - Created a centralized DynamoDB client to optimize Lambda cold starts
   - Added support for SST Resource references for table names
   - Implemented connection reuse for better performance

### Gateway Class Enhancements

1. **Conversation Method Exposure**:
   - Added direct methods to interact with conversations:
     - `createConversation`: Create new conversations
     - `getConversation`: Retrieve existing conversations
     - `listConversations`: List a user's conversations
     - `deleteConversation`: Remove conversations
     - `addMessageToConversation`: Add individual messages

2. **Conversation-Specific Chat Methods**:
   - Implemented `conversationChat` for stateful non-streaming interactions
   - Implemented `conversationStreamChat` for stateful streaming interactions
   - Both methods automatically:
     - Create conversations if they don't exist
     - Load conversation history for context
     - Save user messages and model responses
     - Return conversation IDs

3. **Enhanced Response Types**:
   - Created `ConversationGatewayResponse` to include conversation IDs
   - Ensured proper typing throughout the code

### WebSocket Handler Refactoring

1. **New Action Types**:
   - Implemented `llm/prompt` for stateless interactions
   - Implemented `llm/conversation` for stateful interactions
   - Maintained `llm/chat` as a backward-compatible entry point

2. **Handler Structure**:
   - Refactored to a class-based design with better separation of concerns
   - Implemented proper error handling and logging
   - Centralized WebSocket event processing

3. **Conversation Support**:
   - Added conversation ID generation and tracking
   - Implemented special handling for new vs. existing conversations
   - Added `conversation_info` messages for streaming scenarios

## Key Learnings and Best Practices

1. **Repository Pattern**:
   - Separating data access through interfaces allows multiple implementations
   - Makes testing easier with mock repositories
   - Centralizes data access patterns and validation

2. **DynamoDB Data Modeling**:
   - Using composite keys allows efficient querying of hierarchical data
   - Overloading sort keys provides flexibility in data access
   - Sequence numbers ensure message ordering

3. **Serverless Optimization**:
   - Client reuse is critical for performance
   - Resource references provide deployment flexibility
   - Minimizing cross-service calls reduces latency

4. **Domain-Driven Design**:
   - Keeping conversation logic in the Gateway centralizes business logic
   - WebSocket handlers focus on protocol concerns, not business logic
   - Clear separation of responsibilities increases maintainability

5. **Streaming Patterns**:
   - Token grouping improves network efficiency
   - Separating metadata from content ensures clean interfaces
   - Accumulating content for conversation storage manages state correctly

## Next Steps

1. **Streaming Improvements**:
   - Consider refactoring the provider interface to better support vendor-specific streaming
   - Add proper typing for the Bedrock client in the provider interface
   - Document streaming requirements for each vendor
   - Add tests to verify streaming behavior

2. **Conversation Management**:
   - Implement conversation TTL (time-to-live) for automatic cleanup
   - Add conversation title generation
   - Support message editing and deletion
   - Add conversation forking

3. **Performance Optimization**:
   - Implement batch loading for conversations
   - Add pagination for large conversation histories
   - Consider caching for frequently accessed conversations
   - Optimize DynamoDB read/write units
