# LLM Gateway Design Updates - April 6, 2025

## Current State

### Working Features
- Streaming responses with Claude models via Bedrock
- Basic model configuration and provider setup
- Initial vendor adapter pattern implementation

### Known Issues
- Llama streaming implementation needs fixing (conversation loops)
- Need to distinguish between prompt and chat modes
- Limited provider implementations (only Bedrock)

## Configuration Structure Improvements

### 1. Simplified Model Configuration
- Consolidated all model configurations into a single `models.json` file per provider
- Removed individual model JSON files to reduce duplication and maintenance overhead
- Organized models by vendor in a clear hierarchical structure:
```json
{
  "vendors": [
    {
      "name": "anthropic",
      "models": [
        {
          "modelId": "anthropic.claude-3-sonnet-20240229-v1:0",
          ...
        }
      ]
    }
  ]
}
```

### 2. Provider Configuration Clarity
- Renamed `provider.json` to `settings.json` to better reflect its purpose
- Maintains provider-level settings like:
  - Authentication
  - Region
  - Default parameters
  - Supported vendors
  - Capabilities

### 3. Model Status Tracking
- Added `llmgateway` status information to each model:
```json
"llmgateway": {
  "status": "READY" | "NOT READY",
  "billing": "ONDEMAND" | "provisioned",
  "provisioned": boolean
}
```
- Enables tracking of model availability and billing status
- Helps prevent usage of models that aren't ready for production

## Key Learnings

1. **Vendor Adapter Pattern**
   - Need to distinguish between prompt and chat modes
   - Each vendor requires specific handling of streaming responses
   - Clear separation needed between provider and vendor responsibilities

2. **Streaming Implementation**
   - Different vendors handle streaming differently
   - Need to properly handle conversation history in chat mode
   - Must prevent model self-conversation in streaming responses

3. **Configuration Management**
   - Single source of truth for model configurations
   - Reduced duplication improves maintainability
   - Clear separation between provider settings and model details

4. **Status Tracking**
   - Explicit status tracking helps prevent runtime errors
   - Clear distinction between ready and not-ready models
   - Billing information readily available

## Next Steps

### 1. Core Functionality
- [ ] Implement prompt vs chat mode distinction
- [ ] Fix Llama streaming implementation
- [ ] Add Nova model support with new vendor adapter
- [ ] Test and validate vendor adapter pattern

### 2. Infrastructure
- [ ] Implement secrets and parameter store in SST
- [ ] Add support for Azure and OpenAI API keys
- [ ] Implement additional providers (beyond Bedrock)
- [ ] Test and validate provider pattern

### 3. User Tracking & Analytics
- [ ] Add username tracking in websocket handler
- [ ] Implement parallel API handler
- [ ] Add support for API gateway tags
- [ ] Implement RDS storage for token usage data

### 4. Future Improvements
- [ ] Add more model metadata to suggestions:
  - Model capabilities
  - Pricing information
  - Context window sizes
  - Modality information
- [ ] Implement caching of ready models
- [ ] Add real-time status updates
- [ ] Implement model status monitoring
- [ ] Add usage analytics integration



{"action": "llm/chat", "data": {"messages": [{"role": "user", "content": "Hello, how are you?"}],"provider": "bedrock", "modality": "text", "streaming": true, "modelId": "meta.llama3-8b-instruct-v1:0" } }