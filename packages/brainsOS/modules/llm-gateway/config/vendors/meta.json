{
  "name": "meta",
  "displayName": "Meta",
  "capabilities": {
    "modalities": ["text-to-text"],
    "streaming": true,
    "inferenceTypes": {
      "onDemand": true,
      "provisioned": false
    }
  },
  "apiFormats": {
    "prompt": {
      "format": {
        "prompt": "",
        "max_tokens": 4096
      }
    }
  },
  "modelApiMapping": {
    "llama3-8b-instruct-v1:0": "prompt",
    "llama3-70b-instruct-v1:0": "prompt",
    "llama3-1-8b-instruct-v1:0": "prompt",
    "llama3-1-70b-instruct-v1:0": "prompt",
    "llama3-2-11b-instruct-v1:0": "prompt",
    "llama3-2-90b-instruct-v1:0": "prompt",
    "llama3-2-1b-instruct-v1:0": "prompt",
    "llama3-2-3b-instruct-v1:0": "prompt",
    "llama3-3-70b-instruct-v1:0": "prompt"
  },
  "models": [
    {
      "id": "meta.llama3-2-1b-instruct-v1:0",
      "name": "llama3-2-1b-instruct-v1:0",
      "displayName": "Llama 3 2 1B Instruct",
      "description": "Meta's 1B parameter Llama 3 model optimized for instruction following",
      "contextWindow": 8192,
      "maxTokens": 4096,
      "defaultSettings": {
        "temperature": 0.7,
        "topP": 1,
        "stopSequences": []
      }
    }
  ],
  "responseFormat": {
    "prompt": {
      "completionField": "generation",
      "usageField": "usage"
    }
  },
  "specialHandling": {
    "requiresSystemPrompt": false,
    "supportsStopSequences": true,
    "supportsTemperature": true
  },
  "defaultSettings": {
    "temperature": 0.7,
    "topP": 1,
    "maxTokens": 4096,
    "stopSequences": []
  }
} 